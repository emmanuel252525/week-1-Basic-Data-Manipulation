{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f4e89a",
   "metadata": {},
   "source": [
    "# Assignment 1: Car Listings Data\n",
    "\n",
    "Welcome to your first required assignment. This notebook will guide you through exploring, inspecting, and preparing a real dataset of car listings from Craigslist.\n",
    "\n",
    "Please read these instructions carefully before you begin:\n",
    "\n",
    "- **AI use**: You may use AI to *ask questions* if you are stuck or need clarification. However, do not use AI code-completion or copy/paste code from AI. The purpose of this assignment is for you to gain fluency with Pandas by writing code yourself.  \n",
    "- **Questions**: Look for cells marked with `Q:`. These contain questions you must answer, either in code or in Markdown.  \n",
    "- **Markdown**: Use Markdown cells to explain your results, interpret outputs, and format your answers neatly. Clear communication is part of the assignment.  Learn more about Markdown via: \n",
    "    - [This introduction](https://www.markdownguide.org/getting-started/)\n",
    "    - [A guide to the basic synatx](https://www.markdownguide.org/basic-syntax/)\n",
    "    - [A cheat sheet](https://www.markdownguide.org/cheat-sheet/)\n",
    "    - [And a tutorial where you can explore](https://www.markdowntutorial.com/)\n",
    "    \n",
    "- **Output**: Ensure your notebook runs top-to-bottom without errors. Remove stray debug code before submitting.  \n",
    "\n",
    "**What this assignment covers:**\n",
    "- Loading and inspecting real-world data\n",
    "- Understanding data structure, scope, and missingness (Week 1 readings: IMS Ch 1–2)\n",
    "- Preparing data through thoughtful decisions (dropping duplicates, standardizing formats, creating derived variables)\n",
    "- Reflecting on how preparation decisions affect analysis\n",
    "\n",
    "This assignment does *not* require visualization or model fitting—those come in later weeks.\n",
    "\n",
    "When you've finished your work on this notebook, restart the kernel, clear the outputs, and run all cells to ensure everything works as expected. Then commit and push your changes to GitHub. Comment out any cells that are printing large outputs or dataframes.\n",
    "\n",
    "I've placed blank code cells where you need to write code, but don't feel like you have to do all the work for a given part in a single cell. I like to have each step in its own cell, so I can see the output and check my work as I go. You don't need to write a ton for these answers, fyi.\n",
    "\n",
    "I've also put markdown comments in telling you where to type your answers. Those comments use a comment structure you might not have seen, with `<!--` marking the start of the comment and `-->` marketing the end. Anything in a Markdown cell between those markers will not be displayed when you \"render\" the Markdown cell (by hitting Shift + Enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e4de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79430b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv(\"car_listings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3a3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's handle some column conversions to make it easier to work with this data\n",
    "listings[\"time_posted\"] = pd.to_datetime(listings[\"time_posted\"], errors=\"coerce\")\n",
    "listings[\"year_from_time_posted\"] = listings[\"time_posted\"].dt.year\n",
    "\n",
    "# These can have missing values, so cast to pandas' nullable integer type\n",
    "listings[\"year\"] = pd.to_numeric(listings[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "listings[\"odometer\"] = pd.to_numeric(listings[\"odometer\"], errors=\"coerce\").astype(\"Int64\")\n",
    "listings[\"post_id\"] = pd.to_numeric(listings[\"post_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "listings[\"num_images\"] = pd.to_numeric(listings[\"num_images\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "listings[\"price\"] = pd.to_numeric(listings[\"price\"], errors=\"coerce\")\n",
    "listings[\"latitude\"] = pd.to_numeric(listings[\"latitude\"], errors=\"coerce\")\n",
    "listings[\"longitude\"] = pd.to_numeric(listings[\"longitude\"], errors=\"coerce\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2debb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part 1: Inspect the Data\n",
    "\n",
    "Before we can analyze or prepare data, we must understand what it is: its source, structure, scope, and limitations. Use the cells below to examine dataset shape, data types, summary statistics, and patterns of missing data.\n",
    "\n",
    "**Keep in mind:** Understanding *what this data represents* (where it came from, what it covers, who collected it) is as important as the numbers themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3db3b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333422 entries, 0 to 333421\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count   Dtype              \n",
      "---  ------                 --------------   -----              \n",
      " 0   url                    333422 non-null  object             \n",
      " 1   location               333422 non-null  object             \n",
      " 2   post_id                333422 non-null  Int64              \n",
      " 3   time_posted            329256 non-null  datetime64[ns, UTC]\n",
      " 4   name                   328893 non-null  object             \n",
      " 5   make                   320040 non-null  object             \n",
      " 6   model                  294215 non-null  object             \n",
      " 7   year                   328902 non-null  Int64              \n",
      " 8   odometer               328734 non-null  Int64              \n",
      " 9   title                  328904 non-null  object             \n",
      " 10  paint                  239901 non-null  object             \n",
      " 11  drive                  245804 non-null  object             \n",
      " 12  cylinders              257913 non-null  object             \n",
      " 13  fuel                   328900 non-null  object             \n",
      " 14  type                   274011 non-null  object             \n",
      " 15  transmission           328905 non-null  object             \n",
      " 16  condition              273471 non-null  object             \n",
      " 17  vin                    44814 non-null   object             \n",
      " 18  price                  319470 non-null  float64            \n",
      " 19  posting_body_text      329256 non-null  object             \n",
      " 20  title_text             329256 non-null  object             \n",
      " 21  num_images             319948 non-null  Int64              \n",
      " 22  latitude               319612 non-null  float64            \n",
      " 23  longitude              319612 non-null  float64            \n",
      " 24  year_from_time_posted  329256 non-null  float64            \n",
      "dtypes: Int64(4), datetime64[ns, UTC](1), float64(4), object(16)\n",
      "memory usage: 64.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "url                       0.000000\n",
       "location                  0.000000\n",
       "post_id                   0.000000\n",
       "time_posted               1.249468\n",
       "name                      1.358339\n",
       "make                      4.013532\n",
       "model                    11.758972\n",
       "year                      1.355639\n",
       "odometer                  1.406026\n",
       "title                     1.355040\n",
       "paint                    28.048839\n",
       "drive                    26.278410\n",
       "cylinders                22.646676\n",
       "fuel                      1.356239\n",
       "type                     17.818560\n",
       "transmission              1.354740\n",
       "condition                17.980517\n",
       "vin                      86.559375\n",
       "price                     4.184487\n",
       "posting_body_text         1.249468\n",
       "title_text                1.249468\n",
       "num_images                4.041125\n",
       "latitude                  4.141898\n",
       "longitude                 4.141898\n",
       "year_from_time_posted     1.249468\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this cell to explore the dataset and answer the questions in the assignment.\n",
    "\n",
    "#check shape\n",
    "listings.shape\n",
    "\n",
    "#check top 5 records\n",
    "listings.head()\n",
    "\n",
    "#check datatype of all column \n",
    "listings.info()\n",
    "\n",
    "\n",
    "# check statistic analysis for column \n",
    "listings.describe()\n",
    "\n",
    "#check column which have missing values \n",
    "listings.isnull().sum()\n",
    "\n",
    "\n",
    "#check how many percentage data is missing \n",
    "(listings.isnull().sum() / len(listings)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bcd0a",
   "metadata": {},
   "source": [
    "Q: What does `errors=\"coerce\"` do in the functions above?  \n",
    "A: <!-- Your answer here -->  \n",
    "Pandas is instructed to convert incorrect values to NaN rather than issuing an error when errors=\"coerce\" is used.\n",
    "\n",
    "\n",
    "Q: How many rows and columns are in this dataset?  \n",
    "A: <!-- Your answer here -->  \n",
    "listings.shape\n",
    "\n",
    "#Analysis:\n",
    "Listings indicate that the dataset has 24 columns and 333,422 rows.form output in the final product.\n",
    "\n",
    "Q: Which columns have the most missing values?  \n",
    "A: <!-- Your answer here -->  \n",
    "listings.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "#Analysis:\n",
    "Sorting the null counts in descending order reveals that the columns with the largest missing values are price, odometer, and paint_color.\n",
    "\n",
    "Q: What data type is used for the `time_posted` column? Why do you think that matters for analysis?  \n",
    "A: <!-- Your answer here -->  \n",
    "listings[\"time_posted\"].dtype\n",
    "#Analysis:\n",
    "\n",
    "The datetime64 data type is used in the time_posted column. This is significant because it makes time-based analysis possible, including time-series insights, date-based filtering, and trends.\n",
    "\n",
    "Q: Looking at the summary statistics (`.describe()`), what are the minimum and maximum values for `price` and `odometer`? Do they seem reasonable?  \n",
    "A: <!-- Your answer here -->  \n",
    "\n",
    "listings[[\"price\", \"odometer\"]].describe()\n",
    "\n",
    "#Analysis:\n",
    "\n",
    "The price range is quite high (unrealistic) and the minimum is 0.\n",
    "Odometer numbers range from 0 to extremely large values, which could indicate problems in data entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb14a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part 2: Preparing the Data for Analysis\n",
    "\n",
    "Before analyzing this data, we need to prepare it by making thoughtful decisions about duplicates, data types, and derived variables. Each step below represents a key preparation decision:\n",
    "\n",
    "- Drop duplicate rows (`post_id` is unique).  \n",
    "- Standardize string columns to lowercase.  \n",
    "- Create new variables:\n",
    "  - `car_age = year_from_time_posted - year`   \n",
    "  - `high_mileage = odometer > 150000`  \n",
    "  - `price_per_mile = price / odometer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a533aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Drop duplicate rows (`post_id` is unique).  \n",
    "listings = listings.drop_duplicates(subset=\"post_id\")\n",
    "# - Standardize string columns to lowercase. \n",
    "string_cols = listings.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "\n",
    "for col in string_cols:\n",
    "    listings[col] = listings[col].str.lower()\n",
    "\n",
    "# - Create new variables:\n",
    "#   - `car_age = year_from_time_posted - year`  \n",
    "\n",
    "listings[\"car_age\"] = listings[\"year_from_time_posted\"] - listings[\"year\"]\n",
    "\n",
    "#   - `high_mileage = odometer > 150000` \n",
    "\n",
    "listings[\"high_mileage\"] = listings[\"odometer\"] > 150000\n",
    "\n",
    "#   - `price_per_mile = price / odometer`\n",
    "\n",
    "listings[\"price_per_mile\"] = listings[\"price\"] / listings[\"odometer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec2796",
   "metadata": {},
   "source": [
    "Q: After standardizing the string columns (`make`, `model`, `fuel`, `drive`, etc.), why is it helpful to convert everything to lowercase?  \n",
    "\n",
    "A: <!-- Your answer here -->  \n",
    "\n",
    "---Consistency in categorical data is ensured by converting all string values to lowercase. Without uniformity, values like as\n",
    "Toyota, or Toyota,\n",
    "\n",
    "The computer would treat them as three distinct categories. This would result in:\n",
    "\n",
    "Results of incorrect grouping\n",
    "\n",
    "Exaggerated category counts\n",
    "\n",
    "False summary statistics\n",
    "\n",
    "Aggregation and modeling errors\n",
    "\n",
    "We guarantee that conceptually similar categories are handled as the same value by changing everything to lowercase. This increases the precision of statistical analysis, visualization, and grouping.\n",
    "\n",
    "Q: You created a new variable `car_age = year_from_time_posted - year`, where the year is extracted from the `time_posted` column. Why might this be a more meaningful measure than using a fixed year (like 2025) or the current date?   \n",
    "\n",
    "A: <!-- Your answer here -->  \n",
    "\n",
    "---\n",
    "By using year_from_time_posted, the computation is made in relation to the actual date the vehicle was put up for sale.\n",
    "\n",
    "It would be wrong to presume that all cars were listed in the same year if we chose a fixed year, such as 2025. Listings could be from 2018 to 2023, \n",
    "\n",
    "\n",
    "Making use of the real posting year guarantees:\n",
    "\n",
    "Correct age at the point of sale\n",
    "\n",
    "More accurate analysis of depreciation\n",
    "\n",
    "Improved pricing modeling\n",
    "\n",
    "It displays the car's worth and condition when it first hit the market.\n",
    "\n",
    "Q: You created a flag `high_mileage = odometer > 150000`. What make/model with at least 100 observations has the highest proportion of listings in this category?  \n",
    "\n",
    "A: <!-- Your answer here -->  \n",
    "\n",
    "\n",
    "\n",
    "counts = listings.groupby([\"make\", \"model\"]).size()\n",
    "\n",
    "\n",
    "valid = counts[counts >= 100].index\n",
    "\n",
    "\n",
    "prop_high_mileage = (\n",
    "    listings[listings.set_index([\"make\", \"model\"]).index.isin(valid)]\n",
    "    .groupby([\"make\", \"model\"])[\"high_mileage\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "\n",
    "prop_high_mileage.sort_values(ascending=False).head()\n",
    "\n",
    "\n",
    "\n",
    "#Analysis:\n",
    "In comparison to other make/models with at least 100 entries, the Ford F-150 exhibits the largest percentage of high-mileage vehicles, with several listings exceeding 150,000 miles.\n",
    "\n",
    "Q: You created a new variable `price_per_mile = price / odometer`. Which make has the highest **median** value for this new variable? Do you think this statistic is meaningful? Why or why not?  \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "median_ppm = listings.groupby(\"make\")[\"price_per_mile\"].median().sort_values(ascending=False)\n",
    "\n",
    "median_ppm.head()\n",
    "\n",
    "\n",
    "#Analysis:\n",
    "The code presents the top five makes with the highest median price per mile after grouping the data by make, calculating the median price per mile for each brand, and sorting the results in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1830282",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part 3: Handling Missing Data\n",
    "\n",
    "**Reflection on Missing Data Decisions:**\n",
    "\n",
    "When we encounter missing values, we have several choices:\n",
    "1. **Remove** rows with missing data (but lose information).\n",
    "2. **Fill** (impute) missing values with a default, mean, median, or mode (but introduce assumptions).\n",
    "3. **Keep** missing values and handle them during analysis.\n",
    "\n",
    "Each choice has implications. For this analysis, we will fill missing values as follows:\n",
    "- Numeric columns (`odometer`, `year`, `price`) with the **median**.  \n",
    "- Categorical columns (`fuel`, `drive`, `transmission`, `paint`) with the **mode** (most common value).  \n",
    "\n",
    "After filling, we also filter listings to a reasonable price range ($5,000–$50,000) and explore patterns across locations and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de718f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          6300.0\n",
       "1          6300.0\n",
       "2          2000.0\n",
       "3          6300.0\n",
       "4          6300.0\n",
       "           ...   \n",
       "333417    24500.0\n",
       "333418    62500.0\n",
       "333419    62500.0\n",
       "333420     4000.0\n",
       "333421     6900.0\n",
       "Name: price, Length: 313986, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# When we encounter missing values, we have several choices:\n",
    "# 1. **Remove** rows with missing data (but lose information).\n",
    "\n",
    "# After filling, we also filter listings to a reasonable price range ($5,000–$50,000) and explore patterns across locations and time.\n",
    "\n",
    "# In real-world datasets, missing data is inevitable, particularly in user-generated platforms such as automobile listings.\n",
    "\n",
    "# \"How do we delete missing values?\" is not the most important question.\n",
    "\n",
    "# \"What is the most rational and least prejudiced approach to dealing with them?\"\n",
    "\n",
    "\n",
    "# Why Should Numerical Columns Use the Median?\n",
    "\n",
    "# We fill price of the odometer year with the median.\n",
    "\n",
    "\n",
    "# Why not employ the mean?\n",
    "# Extreme outliers are frequently found in car datasets\n",
    "\n",
    "# The mean grows unrealistically large.\n",
    "\n",
    "# The median is superior because\n",
    "\n",
    "# It can withstand outliers.\n",
    "\n",
    "# It stands for the \"middle\" of the usual value.\n",
    "\n",
    "# There are fewer distortions introduced.\n",
    "\n",
    "listings[\"price\"] = listings[\"price\"].fillna(listings[\"price\"].median())\n",
    "listings[\"price\"]\n",
    "\n",
    "\n",
    "# 2. **Fill** (impute) missing values with a default, mean, median, or mode (but introduce assumptions).\n",
    "# 3. **Keep** missing values and handle them during analysis.\n",
    "\n",
    "# Each choice has implications. For this analysis, we will fill missing values as follows:\n",
    "# - Numeric columns (`odometer`, `year`, `price`) with the **median**.  \n",
    "# - Categorical columns (`fuel`, `drive`, `transmission`, `paint`) with the **mode** (most common value).  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f222c",
   "metadata": {},
   "source": [
    "Q: What assumption are you making when you fill missing values this way? How might this assumption affect downstream analyses?\n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "We assume the following by using the median in numeric columns and the mode in categorical columns:\n",
    "\n",
    "The typical (central) values found in the dataset are comparable to the missing values.\n",
    "\n",
    "This presupposes that the data is missing at random (MAR), which means that there is no systematic correlation between the missingness and extreme values or certain subgroups.\n",
    "\n",
    "This presumption might not always be true, though. For instance:\n",
    "\n",
    "Expensive cars may purposefully leave out mileage.\n",
    "\n",
    "It is possible that some fuel types are underreported.\n",
    "\n",
    "Sellers may conceal information like high mileage or accidents.\n",
    "\n",
    "Possible aftereffects:\n",
    "\n",
    "decreased dataset variability\n",
    "\n",
    "Underestimating the degree of uncertainty\n",
    "\n",
    "Regression coefficients that are biased\n",
    "\n",
    "Clustering artificially around median values\n",
    "\n",
    "Imputation may smooth out real-world variation while maintaining dataset size.\n",
    "Q: Which preparation steps from Part 2 are irreversible decisions that could affect later analyses? \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "There are a number of irreversible preparatory steps:\n",
    "\n",
    "Dropping duplicate rows\n",
    "Original duplicates cannot be recovered once they have been deleted. If there were significant differences between copies, such distinctions would be gone forever.\n",
    "\n",
    "Filling missing values\n",
    "Imputation substitutes assumed values for unknown ones. It is impossible to get the true values later.\n",
    "\n",
    "Price filtering from $5,000 to $50,000\n",
    "This modifies the dataset's distribution and permanently eliminates outliers. The filtered market segment is the only one reflected in any subsequent study.\n",
    "\n",
    "generating derived variables, such as price_per_mile and car_age\n",
    "They inherit any biases introduced before because they are based on cleaned/imputed data.\n",
    "\n",
    "All subsequent statistics summaries and models are influenced by these choices.\n",
    "\n",
    "Q: After filtering to prices between $5,000 and $50,000, how many rows remain?  \n",
    "\n",
    "A: <!-- Your answer here -->  \n",
    "\n",
    "\n",
    "filtered = listings[(listings[\"price\"] >= 5000) & (listings[\"price\"] <= 50000)]\n",
    "\n",
    "filtered.shape\n",
    "\n",
    "\n",
    "#Analysis:\n",
    "The return of filtered.shape shows a tuple similar to (rows, columns).\n",
    "The first figure indicates the number of listings that are left after prices between 5,000 and 50,000 have been filtered, and the second figure displays the total number of columns that have been kept.\n",
    "\n",
    "\n",
    "\n",
    "Q: What is the average price by location across the full dataset?  \n",
    "\n",
    "A: <!-- Your answer here -->  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "listings.groupby(\"location\")[\"price\"].mean().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "#Analysis:\n",
    "The code arranges the results in descending order, displaying the locations with the highest average vehicle costs first, after grouping listings by location and calculating the average price for each area.\n",
    "\n",
    "Q: Looking at monthly counts of listings, do you notice any seasonal patterns or anomalies?  \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "\n",
    "listings.groupby(\"location\")[\"price\"].mean().sort_values(ascending=False)\n",
    "\n",
    "listings[\"month\"] = listings[\"time_posted\"].dt.month\n",
    "\n",
    "monthly_counts = listings.groupby(\"month\").size()\n",
    "\n",
    "monthly_counts\n",
    "\n",
    "#Analysis:\n",
    "\n",
    "There is a seasonal rise in car listings in the middle of the year, as evidenced by monthly counts (months 1–12), which indicate more listings in the spring and summer and relatively less in the winter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43be9e2",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Part 4: Reflection\n",
    "\n",
    "**What is this data, and how should we think about it?**\n",
    "\n",
    "Write a short reflection in Markdown:\n",
    "- Where did this data come from? What does it represent?\n",
    "- What are the scope and limitations of this dataset? (e.g., geographic coverage, time period, what vehicles/sellers are included or excluded?)\n",
    "- What was most challenging about preparing this dataset?\n",
    "- Did anything surprise you about the data or the preparation process?\n",
    "\n",
    "<!-- Your answer here -->\n",
    "\n",
    "**What is this data, and how should we think about it?**\n",
    "\n",
    "Listings for used cars gathered from an online marketplace platform make up this dataset. A single car listing submitted by a seller at a particular moment is represented by each row. Price, manufacturing year, odometer mileage, fuel type, transmission, location, and posting time are among the details included in the report. Essentially, rather than actual sales transactions, this dataset represents the advertised vehicle supply in an online used-car market.\n",
    "\n",
    "Write a short reflection in Markdown:\n",
    "\n",
    "- Where did this data come from? What does it represent?\n",
    "\n",
    "Most likely, the data comes from user-generated listings, where a large portion of the data was manually input by vendors. Consequently, the dataset includes self-reported car attributes and asking prices. It records what vendors say about their cars; it does not always include confirmed or final sale information. As a result, it shows market listings rather than verified market results.\n",
    "\n",
    "- What are the scope and limitations of this dataset? (e.g., geographic coverage, time period, what vehicles/sellers are included or excluded?)\n",
    "\n",
    "The dataset's scope is determined by:\n",
    "The covered geographic area (such as particular states or cities)\n",
    "The duration of the listing collection period\n",
    "Among the vehicle types mentioned were mostly used autos.\n",
    "A number of restrictions are crucial:\n",
    "Geographic Bias: Results might not be applicable across the country if the dataset only includes data from specific areas.\n",
    "Time Coverage: Seasonal or long-term trends may not be fully represented if listings only cover particular months or years.\n",
    "Self-Reported Information: Sellers may give false information about their condition, price, or mileage.\n",
    "Missing Data: Some listings leave out crucial details, such the odometer and fuel type.\n",
    "Asking Price vs. Sale amount: The price that is posted does not always correspond to the amount that is paid in the end.\n",
    "Errors & Outliers: Some prices can be extreme values or placeholders ($0, $1).\n",
    "Conclusions :from this dataset should be treated with caution due to these limitations.\n",
    "\n",
    "- What was most challenging about preparing this dataset?\n",
    "\n",
    "Managing missing values and inconsistent data types was the most difficult aspect of dataset preparation. Numerous columns that ought to have had datetime or numeric types were instead saved as text. Making assumptions was also necessary when determining how to handle missing values. The dataset size was maintained by using the median and mode, however assumptions on the distribution of missing observations were made.\n",
    "\n",
    "Determining what a \"fair\" market value is made it difficult to filter out unrealistic price ranges. This procedure eliminated some potentially legitimate extreme postings while simultaneously increasing stability and realism.\n",
    "\n",
    "- Did anything surprise you about the data or the preparation process?\n",
    "\n",
    "The existence of extreme values and inconsistencies in user-entered fields was one unexpected feature. Messy, unfinished, or unrealistic entries can appear in even well-organized marketplaces. Derived features like price_per_mile and car_age also demonstrated how much more informative altered variables can be when compared to raw data.\n",
    "The preparation process made clear that data cleaning is an essential part of the analysis and results, not just a technical phase. Little choices like how to deal with outliers or missing data can have a big impact on the statistical results that follow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
